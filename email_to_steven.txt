Subject: Sorry for the delay — small calibration/uncertainty demo attached

Hi Steven,

I’m really sorry for the long gap since we last spoke. Midterms and catch‑up work ran longer than I planned. I’m back on a regular cadence now and will keep responses timely going forward.

I interpreted our conversation as: explore whether a transformer’s probabilities behave in a Bayesian‑like way on a simple star task. I built a compact demo that does exactly that and kept it focused so the results are easy to read.

I generated tiny star images with most stars hidden, trained a small transformer to guess which quadrant holds the visible stars, and then checked whether its confidence matches reality. It reaches about 92% validation accuracy. The reliability diagram shows that in the high‑confidence range the model is under‑confident—it is correct more often than it says. I also estimated uncertainty by running the model many times with dropout left on; those repeated runs assign higher uncertainty to mistakes than to correct predictions, which is what we want.

I also spent time reading about Bayesian inference in machine learning and astronomy. Astrometry.net informed the accept/reject framing, Celeste encouraged a probabilistic view of images, Gal & Ghahramani motivated using dropout to approximate uncertainty, and transformer calibration work provided the reliability tools.

The attached notes include a short write‑up with the plots. If this direction looks useful, my next step is temperature scaling to bring stated confidence closer to observed accuracy, and then a simple comparison to a plate solver’s accept/reject signal if you want that baseline.

Thanks for your patience. I’m happy to iterate based on your guidance and focus on what would be most helpful for the lab.

Best,
[Your Name]
